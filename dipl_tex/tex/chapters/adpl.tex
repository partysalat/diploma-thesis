\chapter{AD und Behandlung von Unglattheiten}
 Sämtliche Betrachtungen, welche wir bis jetzt geführt haben, setzen voraus, dass die rechte Seite $F$ unseres Modells \eqref{eq:odemodel} hinreichend glatt definiert ist, d.h. dass mindestens $F\in C^1(\R^n)$ gilt, damit wir sie anwenden können. 

In diesem Kapitel werden Lösungen geführt, welche es ermöglichen, vorherige Betrachtungen auf stückweise lineare\footnote{In dieser Arbeit nutzen wir \glqq linear\grqq~synonym zu \glqq affin\grqq~(oder \glqq polyhedral\grqq), da sich jede affine Funktion in eine lineare Funktion  transformieren lässt. Sie entsprechen den Begriffen, welche bei Griewank \cite{monster} und Scholtes \cite{scholtes2012introduction} verwendet werden.}
Funktionen
% , bzw. lokal Lipschitzstetige Funktionen $F$ 
anzuwenden. Desweiteren wird eine kurze Einführung im Automatischen Differenzieren gegeben, welches so erweitert wird, dass es auf verkettet stückweise lineare Funktionen angewendet werden kann.
\section{Verallgemeinerte Gradienten}
Da wir im späteren Verlauf der Arbeit spezielle unglatte Funktionen betrachten wollen, muss zunächst geklärt werden, wie ein möglicher Ableitungsbegriff aussieht und welche Eigenschaften für diesen gelten.

Deshalb betrachten wir zuallererst die Klasse der lokal lipschitzstetigen Funktionen. Eine Funktion $F$ ist \textit{lokal lipschitzstetig}, falls eine Umgebung $V\subseteq U$ zu einem $x\in U$ existiert, sodass $F$ eingeschränkt auf diese Umgebung $V$ lipschitzstetig ist.
Es stellt sich heraus, dass diese Funktionenklasse fast überall differenzierbar ist \cite[S.216 ff]{federer1969geometric}
\begin{theorem}[Rademacher]
\label{thm:rademacher}
 Sei $F:\R^n\to \R^m$ lokal lipschitzstetig auf einer Menge $\mathcal D\subseteq \R^n$. Dann existiert die Fréchet Ableitung 
 \[
 \begin{aligned}
  F'(x)\in \R^{m\times n} \text{ mit } \|F'(x)\|  \leq Lip_f(x) & \text{ s.d. }\\
   \lim_{s\to 0}\frac{\|F(x+s)-F(x)-F'(x)s}{\|s\|} = 0&
  \end{aligned}
 \]
für alle $x\in \mathcal D\backslash S$, wobei $S$ eine Nullmenge ist und $Lip_F(x)$ die lokale Lipschitzkonstante von $F$ um $x$ bezeichnet.
\end{theorem}

Für die Elemente aus der Nullmenge $S$ existiert jedoch nun keine Fréchet Ableitung. Schaut man sich jedoch Richtungsableitungen an, so kann man das klassiche Konzept der Ableitungen erweitern. Eine Funktion, die aus einer offenen Umgebung $U\subseteq \R^n$ nach $\R^m$ abbildet wird \textit{richtungsdifferenzierbar} genannt, falls für jedes $d\in \R^n$ der Grenzwert
\begin{equation}\label{eq:bouligandDerivative}
 F'(x_0;d) = \lim_{\substack{s\to 0 \\ s>0}} \frac{F(x_0+sd)-F(x_0)}{s} 
\end{equation}
existiert. $F'(x_0;d)$ heißt hierbei die Richtungsableitung von $F$ in $x_0$ in Richtung $d$.

\begin{definition}[B-Differenzierbarkeit]
Eine Funktion, welche sowohl lokal lipschitzstetig als auch richtungsdifferenzierbar ist wird Bouligand differenzierbar oder B-differenzierbar genannt (\cite[Def.3.1.2]{scholtes2012introduction}). 
\end{definition}

 Anders ausgedrückt bedeutet dies, dass die Funktion lokal lipschitzstetig und die Funktion $G(x) = F(x)+F'(x_0;x-x_0)$ eine Approximation erster Ordnung der Funktion $F$ sein muss. $F'(x_0;\cdot)$ heißt hierbei \textit{B-Derivative} oder Bouligandableitung.

Für die Bouligandableitung gelten alle Differentiationsregeln wie für die Fréchetableitung \cite[Cor.3.1.1.]{scholtes2012introduction}. Dies gilt insbesondere für die Kettenregel \cite[Thm. 3.1.1.]{scholtes2012introduction}
\begin{equation}
 \label{eq:bouligandChainRule}
 (F\circ G)'(x_0;d) = F'(G(x_0);G'(x_0;d)) 
\end{equation}

Ein weiterer allgemeiner gefasster Ableitungsbegriff ist die Clarke Ableitung \cite[Def. 2.6.1.]{clarke1990optimization}, welche mengenwertige Funktionen als Ableitungen zulässt.
\begin{definition}[Limiting Jacobian und Clarke Ableitung]
 Sei die Funktion $F$ und die Mengen $\mathcal D$ und $S$ wie im Theorem von Rademacher \ref{thm:rademacher} definiert. $\nabla F(y)$ sei die Jacobimatrix an einer Stelle $y\in \mathcal D\backslash S$. Dann ist die Menge der \textit{limiting Jacobians} und die \textit{verallgemeinerte Ableitung nach Clarke} definiert als
 \begin{align}
  \partial^L F(x) &= \lbrace \lim_{i\to \infty} \nabla F(x_i)~ \vert~ x_i \to x, x_i \not \in S \rbrace\\
  \partial F(x) &= \text{conv} \lbrace \partial^L F(x)  \rbrace
 \end{align}
%  mit "conv als konvexe Hülle.
\end{definition}
Die Menge der limiting Jacobians $\partial^L F(x)$ beschreibt also die Menge der geeigneten Fréchetableitungen in der Umgebung von Auswertungspunkt. Die Clarke Ableitung $\partial F(x)$ ist die konvexe Hülle darüber.

% Eine Funktion $F:U\subseteq \R^n\to \R^m$ heißt lipschitzstetig, wenn eine Konstante $L$ existiert, sodass für alle $x,y\in U$ gilt
% \[
%  \|F(y) - F(x)\| \leq \|y-x\|
% \]
% , wobei $L$ als \textit{lipschitzkonstante} bezeichnet wird. 
% Eine Funktion $F$ wird \textit{lokal lipschitzstetig} genannt, falls eine Umgebung $V\subseteq U$ zu einem $x\in U$ existiert, sodass $F$ auf eingeschränkt auf diese Umgebung $V$ lipschitzstetig ist.

\section{Stückweise lineare Funktionen}
\subsection{Max-Min Repräsentation}
\label{sec:maxMinRepresentiation}
Scholtes definiert in \cite[S.19]{scholtes2012introduction} eine stückweise affine Funktion $f:\R^n\to \R^m$ als eine stetige Funktion, zu der eine endliche Menge affiner Funktionen $f_i(x)=A_ix+b_i$, $i=1,\ldots,k$ existiert, sodass für jedes $x\in \R^n$ die Inklusion $f\in\lbrace f_1(x),\ldots, f_k(x)\rbrace $ gilt und $f$ global stetig ist. Die affinen Funktionen $f_i$ werden \textit{Auswahlfunktionen} genannt. 
Auswahlfunktionen für die $F(x) = F_i(x)$ an einem Punkt $x\in R^n$ gilt werden \textit{aktiv} genannt. Ist dies nicht der Fall, so wird sie als \textit{passiv} bezeichnet.
Scholtes bewies in \cite[Prop.2.2.2.]{scholtes2012introduction}, dass sich jede dieser stückweise affinen Funktionen als Verkettung der Funktionen max und min darstellen lässt.
\begin{theorem}[Max-Min Repräsentation]
 Falls $f:\R^n\to \R^m$ eine stückweise affine Funktion mit affinen Auswahlfunktionen $f_1=A_1^\tr x+ A_1,\ldots,f_k=A_k^\tr x+ b_k$ ist, dann existiert eine endliche Anzahl von Indexmengen $M_1,\ldots,M_k\subseteq \lbrace 1,\ldots,k\rbrace$ sodass
 \begin{equation}
 \label{eq:maxMinRepresentiation}
  f(x) = \max_{1\leq i\leq l} \min_{j\in M_i} a_i^\tr x + b_i
 \end{equation}
 
\end{theorem}
In der Theorie hat diese Repräsentation viele Vorteile. In der Praxis stellt es sich jedoch als aufwändig heraus, eine Max-Min Repräsentation zu einer gegebenen stückweise affinen Funktion zu finden. Desweiteren ist sie offensichtlich nicht eindeutig. Falls eine Repräsentation gefunden wurde ist es numerisch sinnvoll, eine minimale Verschachtelungstiefe zu erreichen, d.h. möglichst wenig $min$/$max$ Aufrufe miteinander zu verschachteln. 
% Diese zu reduzieren gestaltet sich im Allgemeinen ebenfalls schwierig.

Eine andere Art der Darstellung, welche in der polyhedralen Natur der stückweise lineare Funktionen liegt, wird durch eine endliche Menge von Teilmengen $\Sigma\subset \R^n$ definiert, auf denen die stückweise affine Funktion mit ihren Auswahlfunktionen übereinstimmt. Es lässt sich zeigen, dass sich diese Menge in konvexe Polyeder zerlegen lässt, so dass ihre Auswahlfunktionen auf einem oder mehreren Polyeder oder Schnittmengen mehrerer Polyeder aktiv sind \cite[S.23 ff.]{scholtes2012introduction}.
 Eine Menge $P\subseteq \R^n$ wird Polyeder genannt, falls eine $m\times n$ - Matrix $A$ und ein $m$-dimensionaler Vektor $b$ existiert, sodass $P = \lbrace x\in \R^n ~|~ Ax\leq b \rbrace$. 
Die Menge dieser konvexen Polyeder werden \textit{Polyhedrale Subdivision} von $\R^n$ zur Funktion $f$ genannt. Das heißt, dass $\Sigma$ eine Polyhedrale Subdivision von $P \subseteq \R^n$ ist, falls
\begin{enumerate}
 \item jedes Polyeder $P_\sigma \in \Sigma$ eine Teilmenge von $\R^n$ ist.
 \item die Dimension der Polyeder aus $\Sigma$ mit der Dimension von $\R^n$ übereinstimmt.
 \item die Vereinigung aller Polyeder von $\Sigma$ den Raum $\R^n$ überdeckt.
 \item jeweils zwei Polyeder von $\Sigma$ sind entweder disjunkt oder berühren sich nur an ihren Kanten.
\end{enumerate}
\begin{figure}[H]
\centering
\input{img/polyhedral_subdevision.tikz}
\caption{Polyhedrale Subdivision}
\label{fig:polyhedralSubdivision}
\end{figure}
Die Berührungskanten aus Punkt 4, bzw. ihrere höherdimensionalen Äquivalente werden als \textit{Kinks} bezeichnet. In Abbildung \ref{fig:polyhedralSubdivision} wird eine mögliche polyhedrale Subdivision gezeigt. Die $P_\sigma$ sind dabei die Polyeder in der Urbildmenge.
Scholtes zeigt in \cite[Prop. 2.2.3.]{scholtes2012introduction}, dass jede stückweise affine Funktion eine zugehörige Polyhedrale Subdivision besitzt.

\subsection{Abs-Normal-Form}
\label{sec:absNormalForm}
Eine weitere Art der Darstellung ergibt sich aus der Max-Min Repräsentation \eqref{eq:maxMinRepresentiation} einer stückweisen linearen Funktion.
Da sich $max$ und $min$ als
\[
\max(a,b) = \frac{1}{2}(a+b + |a-b|)\quad \text{und} \quad \min(a,b) = \frac{1}{2}(a+b - |a-b|)
\]
darstellen lassen, können alle $max$ und $min$ - Funktionsaufrufe als $s\geq 0$ Aufrufe der $|z_i|$ Funktion behandelt werden. Die Argumente $z_i$ heißen hierbei \textit{switching-Variablen}. Jede switching Variable $z_i$ beschreibt für sich genommen bereits eine stückweise lineare Funktion, welche wiederrum die Funktionen $|z_j|, ~j<i$, aufrufen. Desweiteren existieren Variablen $x_k, ~k\leq n$, welche unabhängig von $abs$ sind. Diese Abhängigkeiten lassen sich in der \textit{Abs Normal Form} beschreiben, welche von Griewank et. al. in \cite{plan} eingeführt wurde:
\begin{definition}[Abs Normal Form]
 Eine stückweise lineare Funktion $F:\R^n\to \R^m$ mit $F(x) = y$ ist in \textit{Abs Normal Form}, falls sie sich darstellen lässt als
 \begin{equation}
\label{eq:absNormalForm}
  \begin{bmatrix}
   z\\y
  \end{bmatrix}
  =
  \begin{bmatrix}
   c\\b
  \end{bmatrix}
  +
  \begin{bmatrix}
   Z & L\\
   J & Y
  \end{bmatrix}
  \begin{bmatrix}
   x\\|z|
  \end{bmatrix}
 \end{equation}
wobei gilt:
\[
c\in \R^s, ~ Z\in \R^{s\times n},~ L\in\R^{s\times s}, ~ b\in\R^m, ~ J\in\R^{m\times n}, ~ Y\in \R^{m\times s} 
\]
% Die Vektoren $c$ und $b$ 
$L$ muss offensichtlich die Form eine streng unteren Dreiecksmatrix besitzen, da sonst im Auswertungsgraph der Funktion $F$ Zyklen entstehen könnten. Die Zahl $s\in \N$ ist hierbei die Gesamtanzahl der $abs$ Aufrufe.
\end{definition}
Die kleinste ganze Zahl $\nu \leq s$ für die 
\[
 L^\nu =0
\]
ist, wird Verschachtelungstiefe oder \textit{switching depth} genannt. 
In der numerischen Praxis kommen Beispiele vor, bei denen eine hohe \textit{switching depth} auftreten kann, beispielsweise bei Flux Limitern bei ortsdiskretisierten PDEs, welche wir im Beispiel (siehe Kapitel \ref{sec:fvmFluxEigen}) zur Shallow Water Equation benutzen. Griewank erläutert in \cite{monster}, dass eine große switching depth zu numerischen Instabilitäten führen kann und man daher versuchen soll, diese möglichst gering zu halten, um zudem auch die kombinatorische Komplexität der Matrizen $Z$, $L$ und $Y$ zu minimieren. Tatsächlich lässt sich die switching depth $\nu$ der Abs-Normal Repräsentation für eine stückweise lineare Abbildung $F:\R^n\to\R^m$ durch 
\[
\nu \leq \bar \nu(n) = 2n-1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \]
beschränken (\cite[S.3]{plan})


Unsere Funktion $F$ ist linear, falls $\nu=s = 0$ und sie somit keine $abs$ Terme besitzt. Für den Fall $\nu=1$ heißt $F$ \textit{simply switched}
\subsection{Verkettet stückweise glatte Funktionen}
Ziel dieser Arbeit ist es, die Eigenschaften der Matrizen der Abs-Normal Formulierung für unsere Berechnungen zu nutzen.
Dazu wollen wir die Klasse der \textit{composite piecewise smooth functions} (also verkettet stückweise glatte Funktionen) durch stückweise lineare Funktionen approximieren.
Die Klasse der verkettet stückweise glatten Funktionen kann als Erweiterung der stückweisen affinen Funktionen betrachtet werden. Sie bestehen aus $C^{1,1}$ Auswahlfunktionen, welche durch wenige unglatte Operationen $max$, $min$ und $abs$ stetig verknüpft sind.
Scholtes nennt solche Funktionen stückweise differenzierbar, falls  zusätzlich die unglatten Operationen höchstens abzählbar oft auf dem $\R^n$ jedoch nur endlich oft auf dem beschränkten Gebiet $U$ auftreteten. Damit ist jede verkettet stückweise glatte Funktionen stückweise differenzierbar und damit lokal lipschitzstetig \cite[Cor. 4.1.1.]{scholtes2012introduction}. Dies wiederrum impliziert, dass solche Funktionen ebenfalls Bouligand differenzierbar sind.

Die grundsätzliche Idee besteht nun darin, die Auswahlfunktionen durch Tangenten anzunähern und sie wieder durch Anwendung der $abs$ Funktion zu verbinden, sodass eine stückweise Linearisierung der Ursprungsfunktion entsteht. Für diese gelten besondere Eigenschaften, welche in den nachfolgenden Kapiteln sukzessive angewendet werden.

Um die stückweise Linearisierung einer stückweise glatten Funktion $F$ automatisiert zu berechnen, wollen wir die Möglichkeiten des Automatischen Differenzierens nutzen und erweitern.

\section{Automatisches Differenzieren}
Automatisches oder Algorithmisches Differenzieren ist ein Verfahren, um, im Rahmen der Rechengenauigkeit, exakte Gradienten von Funktionen zu berechnen (\cite[S.51]{griewank2008evaluating}). Um dies zu erreichen wird weder numerisch noch algebraisch, sondern durch wiederholte Anwendung der Kettenregel die Ableitung berechnet. Dadurch wird eine verschachtelte Funktion auf einzelne Funktionen heruntergebrochen, von denen die Ableitung bekannt ist. Das bedeutet, dass wir durch einen Aufwand, der durch ein Vielfaches einer Funktionsauswertung beschränkt ist, den Gradienten einer Funktion an einer Stelle berechnen können (\cite[S. 43, S.83]{griewank2008evaluating}). 

Um eine vektorwertige Funktion $F:\mathcal D \subset \R^n\to \R^m$ mittels Automatischem Differenzieren abzuleiten, muss sie bereits in einer algebraischen Form vorliegen, sie soll also aus Verknüpfung der Relationen $+, -, \cdot, /$ oder polynomiellen arithmetischen Operationen sin, cos, sqrt, exp, log usw. bestehen, also jenen, von denen eine elementare Ableitung bekannt ist.  

Zum Auswerten einer programmierten Funktion führt der Compiler eine sogenannte \textit{Three-Part Evaluation Procedure} durch, d.h. er speichert die unabhängigen Eingangsvariablen $x\in \R^n$ in internen Variablen $(v_{1-n},\ldots,v_{-1})$ ab, führt $l-m$ Berechnungsschritte $(v_1,\ldots, v_{l-m})$ aus und gibt die abhängigen Variablen $(v_{l-m+1},\ldots,v_l)$ zurück. Diese Berechnungen lassen sich durch einen \textit{Computational Graph} visualisieren, wie dies in Abbildung \ref{fig:computationalGraph} für das Beispiel $F(x) = \sin(|x_1\cdot x_2|+x_1)$ durchgeführt wurde. 
Die \textit{Elementarfunktionen} $\lbrace \phi_i \rbrace_{i\in I}$ werden nacheinander in der richtigen Reihenfolge ausgeführt, welche durch die Halbordnung $\prec$ definiert ist, sodass wir für jedes $v_i$ mit $i>0$ und Argumenten $v_j$, $j<i$ schreiben können
\[
 v_i = \phi_i(v_j)_{j\prec i}
\]

Um nun die erste Ableitung einer Funktion zu bestimmen, müssen wir jede ihrer Elementarfunktionen ableiten und sie mit der Kettenregel wieder zusammenfügen. Falls am Punkt $\xo$ die Ableitung in Richtung eines Inkrements $\Delta x = x- \xo $ ausgewertet werden soll, dann erhalten wir eine tangentiale Approximation der Ableitung unter Nutzung des Funktionswertes $\vo_i = v_i(\xo)$ mittels 
\begin{equation}
\label{eq:plApproximation}
v_i(\xo - \Delta x) -\vo \approx \Delta v_i \equiv \Delta v_i(\Delta x) 
\end{equation}
Für alle $\phi_i$ wird angenommen, dass sie differenzierbar in einer für uns hinreichenden Umgebung sind. Dann werden die tangentialen Linearisierungen
\begin{equation}
\begin{aligned}
\Delta v_i &= \Delta v_j \pm \Delta v_k & \text{für } v_i = v_j \pm v_k\\
\Delta v_i &= \vo_j* \Delta v_k +\Delta v_j*\vo_k& \text{für } v_i = v_j * v_k\\
\Delta v_i &= \mathring c_{ij} * \Delta v_j  & \text{für } v_i = \phi_i(v_j) \not \equiv \abs(v_j)
\end{aligned}
\label{eq:stdAdRules}
\end{equation}

benutzt, wobei $\mathring c_{ij} = \phi'_i(\vo_j)$.
% und als \textit{tangent functions} bezeichnet werden 
Daher ergibt sich durch die Kettenregel \eqref{eq:bouligandChainRule}
\[
 \Delta y = \Delta F(\xo,\Delta x) \equiv \nabla F(\xo)\Delta x
\]
bei der $\nabla F(\xo)\in \R^{m\times m}$ die Jacobimatrix bezeichnet.

Diese Methode wird \textit{forward mode} genannt, bei welchem insbesondere für Ableitungen höherer Dimensionen $d$ der Aufwand linear mit $d$ wächst. Um diesen Aufwand zu verkleinern existiert in Analogie zum forward mode der sogenannte \textit{reverse mode}, welcher die Zielfunktion auswertet, Parameter abspeichert und die Ableitung mit weniger Aufwand aus den gespeicherten Werten \textit{rückwärts} berechnen kann. Diese Möglichkeit wird an dieser Stelle nur erwähnt, da ihre Erläuterung den Rahmen dieser Arbeit übersteigt.

Automatisches Differenzieren wurde bisher nur für Elementarfunktionen benutzt, die eine Ableitung besitzen. Im nächsten Kapitel werden wir dies auf $\abs(x)$ erweitern.


\section{Stückweise Linearisierung}
Wie zuvor bereits erwähnt liegt unser Ziel nun darin, verkettet stückweise glatte Funktionen zu approximieren. Eine verkettet stückweise glatte Funktion besteht nur aus $C^{1,1}$ Elementarfunktionen $\phi_i$ und $abs$ Aufrufen. Diese Klasse von Funktionen sind häufig in klassischen Problemfeldern zu finden.

In den AD-Regeln der bisherigen Tangentenlinearisierungen \eqref{eq:stdAdRules} werden $abs$-Terme nicht betrachtet. Um diese zu erweitern wird
die stückweise lineare Richtungsfunktion $F(\xo + \Delta x) - F(\xo) = \abs(\xo+\Delta x) - abs(x)$ benutzt und damit \eqref{eq:stdAdRules} um die Regel
\begin{equation}
\label{eq:absAdRule}
\begin{aligned}
 \Delta v_i &= \abs(\vo_j+ \Delta v_j) - \vo_i &\text{für } v_i = \abs(v_j)
 \end{aligned}
\end{equation}

erweitert. Exemplarisch wird die Auswertungs mit der erweiterten Regel anhand der Beispielfunktion $\sin(|x_1\cdot x_2|+x_1)$ in Tabelle \ref{fig:computationalGraphTable} durchgeführt.
\begin{figure}
\centering
 \input{img/computational_graph.tikz}
 \caption{Computational Graph für $F(x) = \sin(|x_1\cdot x_2|+x_1)$}
\label{fig:computationalGraph} 
 \begin{tabular}{|l|l||c|c|}
 \hline
 $v_{-1} =  x_1$ 		&$\Delta v_{-1} = \Delta x_1$ & -2 & 1	\\
 $v_{0} =  x_2$ 		&$\Delta v_{0} = \Delta x_2$&1 &-1	\\
 \hline
 $v_{1} = v_{-1}\cdot v_{0}$ 	&$\Delta v_{1} = \Delta v_{0}v_1 + v_0 \Delta v_{1}$&-2&4\\
 $v_{2} =  \abs(v_{1})$		&$\Delta v_{2} =  |v_1 + \Delta v_1| - v_2$&2&0\\
 $v_{3} =  v_2+v_{-1}$ 		&$\Delta v_{3} = \Delta v_2 + \Delta v_0$&2&-1\\
 $v_{4} =  \sin(v_{3})$ 	&$\Delta v_{4} = \cos(v_3) \Delta v_3$&$\approx 0.90929$& $\approx 0.416146$ \\
 \hline
 $y =  v_{4}$ & $\Delta y = \Delta v_4$&$\approx 0.90929$& $\approx 0.416146$ \\
 \hline
\end{tabular}
\caption{Three Part Evaluation Procedure für $F(x) = \sin(|x_1\cdot x_2|+x_1)$ an der Stelle $\xo = (-2,1)$ in Richtung $\Delta x = (1,-1)$}
\label{fig:computationalGraphTable}
\end{figure}
Für eine Auswertungsprozedur unter Berücksichtigung der erweiterten Regel \eqref{eq:absAdRule} ergibt sich für $\Delta y$ \cite[S.9]{monster} die Inkrementfunktion.
\begin{definition}[Inkrementfunktion]
\label{def:incrementFunction}
Gegeben sei eine verkettet stückweise glatte Funktion $F:\R^n\to\R^m$ mit $s>0$ Abs- Aufrufen. Für eine offene Umgebung $\mathcal D$ einer abgeschlossenen konvexen Definitionsmenge $\mathcal K \subset \R^n$, einem fixen $x\in \mathcal D$, einem Entwicklungspunkt $\xo$ und einer Richtung $\Delta x = x-\xo$ heißt 
\[
\Delta y = \Delta y (\Delta x) = \Delta F(\xo,\Delta x):\R^n \to \R^m
\]
die stückweise lineare und stetige \textit{Inkrementfunktion}.
\end{definition}

Für stückweise differenzierbare Funktionen ist die Bouligandableitung $F'(\xo;\Delta x)$ stückweise linear und positiv homogen, d.h. es gilt $F'(\xo;\alpha \Delta x) = \alpha F(\xo;\Delta x), \alpha\geq 0$, was die Inkrementfunktion jedoch nicht ist  (siehe \ref{sec:bouligandAndIncrement}).
Tauscht man \eqref{eq:absAdRule} durch die Vorschrift
\begin{equation}
 \label{eq:improvedAbsAdRule}
 \begin{aligned}
  \Delta v_i &= (\vo_j \neq 0)?\sign(\vo_j)\Delta v_j:\abs(\Delta v_j) &\text{für } v_i = \abs(v_j)
 \end{aligned}
\end{equation}

aus, mit
\[
 \sign(x) = \begin{cases}
            1 &, x>0\\
            -1&, x<0\\
            0 &, \text{sonst}
           \end{cases}
\]
und falls das Inkrement $\Delta x$ hinreichend klein ist, existiert eine Grenze $\rho(\xo)$ sodass für die Inkrementfunktion 
 \[
  \Delta F(\xo;\Delta x) = F'(\xo;\Delta x)\text{ falls }\|\Delta x\|\leq \rho(\xo)
 \]
gilt  (\cite[S.10 ff]{monster}). Das bedeutet, dass die Inkrementfunktion und die Bouligandableitung in einer Umgebung des Entwicklungspunkts $\xo$ übereinstimmen. Demnach gilt für die Inkrementfunktion $\Delta F$ ebenfalls die Kettenregel \eqref{eq:bouligandChainRule} in einer kleinen Umgebung von $\xo$. Desweiteren ist zu beachten, dass die Bouligandableitung durch \eqref{eq:improvedAbsAdRule} unstetig in Bezug zu $\xo$ mit fixem $\Delta x$ wird. 

Betrachtet man nun die Funktion $F(\xo)+\Delta F(\xo, x-\xo)$ so ergibt sich sogar eine Approximation der Ursprungsfunktion zweiter Ordnung im Abstand zum Ausgangspunkt, wie Griewank in \cite[Prop.1]{monster} zeigen konnte:

\begin{theorem}[Quadratische Approx. und lipschitzstetigkeit]
\label{thm:quadrApproxPL}
 Sei $F$ eine verkettet stückweise glatte, lipschitzstetige, differenzierbare Funktion auf einer offenen Umgebung $\mathcal D$ einer abgeschlossenen konvexen Definitionsmenge $\mathcal K \subset \R^n$. Dann existiert eine Konstante $\gamma$, sodass für alle Paare $\xo,x\in \mathcal K$
 \[
  \|F(x) - F(\xo) - \Delta F(\xo;x-\xo)\| \leq \gamma \|x-\xo\|^2
 \]
 Desweiteren gilt für jedes Paar $\tilde x,\xo\in \mathcal K, \Delta x\in \R^n$ und einer Konstanten $\tilde\gamma$
 \[
  \frac{\|\Delta F(\tilde x;\Delta x) - \Delta F(\xo;\Delta x)\|}{1+\|\Delta x\|}\leq \tilde \gamma \|\tilde x - \xo\|
 \]
\end{theorem}
Da sich das Inkrement, also die Ableitung an der Stelle $\xo$ genau berechnen lässt, ist $F(\xo)+\Delta F(\xo, x-\xo)$ also eine Tangente. Die stückweise Linearisierung, welche diese Tangente benutzt wird von Griewank auch als \textit{tangent mode} bezeichnet. 

Wie in Abbildung \ref{fig:piecewiseLinearization} zu sehen, werden von der Ursprungsfunktion $F = \max(F_1,F_2)$ die Tangenten der Auswahlfunktionen gebildet und diese wieder durch $\max$ konkateniert. Es entsteht die stückweise Linearisierung. Offensichtlich stimmen die Kinks der stückweisen Linearisierung mit der Ursprungsfunktion nicht genau überein. Sie bieten jedoch wie bereits erwähnt eine Approximation zweiter Ordnung. 

\begin{figure}
\centering
 \input{img/piecewise_linearization.tikz}
 \caption{Stückweise Linearisierung im Tangentmodus}
\label{fig:piecewiseLinearization} 
\end{figure}


Der Signaturvektor, welcher als 
\[
 \sigma = \sigma(\Delta x) \in \lbrace -1,0,1\rbrace^s
\]
definiert ist, beschreibt den Weg des computational graphs bei der Auswertung der Inkrementfunktion, d.h. durch jeden $abs$ Aufruf wird beispielsweise durch Vorzeichenwechsel definiert, dass ein Kink überquert wurde. Die Mengen 
\[
 P_\sigma = \lbrace \Delta x \in \R^n:\sigma(\Delta x) = \sigma \rbrace
\]
sind relativ offen und konvexe Polyeder in $\R^n$, welche paarweise disjunkt sind und den kompletten $\R^n$ aufspannen.
Die polyhedrale Dekomposition der Auswertungsprozedur ist dabei eindeutig durch diesen Signaturvektor definiert.

Unser stückweise Lineares Modell besitzt die folgenden Eigenschaften
\cite[Prop. 2]{monster}
\begin{theorem}[Stückweise Lineares Modell]
 \begin{enumerate}
  \item An jedem $\xo\in \mathcal D$ ist die Funktion $\Delta F(\xo;\Delta x)$ definiert für alle $\Delta x\in \R^n$.
  \item $\R^n$ ist die disjunkte Vereinigung relativ offener Mengen konvexer Polyeder $P_\sigma$ für $\sigma \in \lbrace -1,0,1\rbrace$
  \item Auf dem Abschluss $\bar P_\sigma$ ist die Funktion $\Delta F(\xo;\Delta x)$ linear mit Jacobimatrizen $J_\sigma\in \R^{m\times n}$
  \item Wenn eine gemeinsame Seite $\bar P_\sigma \cap S_{\tilde \sigma}$ die maximale Dimension $n-1$ besitzt, dann gilt $J_\sigma - J_{\tilde \sigma} = 2ba^\tr$, wobei $a$ eine Normale auf der Seite ist, welche nicht null ist und $b\in \R^n$
 \end{enumerate}
\end{theorem}



% Ersetzt man nun diese verkettete stückweise glatten Funktionen durch stückweise lineare Funktionen, so erhält man eine Approximation der Funktion zweiter Ordnung im Abstand zum Ausgangspunkt (\cite[Prop.1]{monster}). Man definiert dafür für einen Punkt $\xo$ und eine Richtung $\Delta x$ die \textit{Inkrementfunktion} $\Delta F(\xo,\Delta x):\mathcal D \times  \R^n \to \R^m$ für welche gilt:
% \[
%  F(\xo + \Delta x) = F(\xo) + \Delta F(\xo,\Delta x) + \mathcal O (\|\Delta x\|^2)
% \]

Diese Ergebnisse auf die Datenassimilierung anzuwenden wird der Schwerpunkt der nächsten beiden Kapitel sein.
% \section{Alternative Methoden?}
% \subsection{Event Handling?}
% \subsection{Schrittweitensteuerung?}