\chapter{Experimente}
In diesem Kapitel werden wir drei verschiedenen Beispielen mit sämtliche eingeführten Methoden duchrechnen. Dabei wird zunächst auf die Lösung der Differentialgleichung eingegangen und Konvergenzbetrachtungen durchgeführt. Danach wird der Gradienten des Kostenfunktionals behandelt und die Optimierung für diverse Anfangswerte durchgeführt.
Als erstes Problem dient das Rolling Stones Beispiel, welche ein stückweise lineares Problem darstellt. Die LC- Diode, das nächste Beispiel, ist ein System von gewöhnlichen Differentialgleichungen, welche nahe der Maschinengenauigkeit modelliert. Als Abschluss wird die 1-D Shallow Water Equation Ortsdiskretisiert, sodass sie mit der verallgemeinerten Mittelpunktsregel als System von ODEs in der Zeit gelöst werden kann.
%Stay Tuned!
\section{Rolling Stone}
% \subsection{Problemstellung}
Zum Anfang wollen wir uns dem sogenannten Rolling Stones Beispiel widmen, welches beispielsweise in \cite{boeck2014experiments} oder \cite{hasenfelder13} eingeführt wurde. 
Es behandelt eine sich reibungslos bewegende Kugel auf einer konvexen Parabel, in dessen Mitte eine flache Ebene auf dem Intervall $[-1,1]$ eingefügt wurde. 
\begin{figure}[ht]
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_stones.tikz}
\caption{Rolling Stones}
\label{fig:rollingStones}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_stones_solution.tikz}
\caption{Rolling Stones Lösung}
\label{fig:rollingStonesSolution}
\end{minipage}
\end{figure}
Figur \ref{fig:rollingStones} zeigt die Rampe
\[
 V(z) = \left(\frac{(1+z)^2}{2}\right)\chi_{(-\infty,-1]} + \left(\frac{(1-z)^2}{2}\right)\chi_{[1,\infty)} ,
 ~ \chi_{[a,b)}(z) = 
 \begin{cases}
  1 & z \in [a,b)\\
  0 & \text{sonst}
 \end{cases}
\]

und deren Ableitung, welche wir als gewöhnliche Differentialgleichung
 \begin{equation}
  \begin{pmatrix}
   \dot x_1 \\
   \dot x_2 \\
  \end{pmatrix}
 = 
 \begin{pmatrix}
  x_2 \\
  -x_1 - \frac{|x_1-1|}{2} + \frac{|x_1+1|}{2}
 \end{pmatrix}
=F(x)
\label{eq:rolling_stones}
 \end{equation}
auffassen, wobei $x_1$ die Abszissenposition des Steines und $x_2$ dessen Geschwindigkeit bezeichnet.
Die Funktion \eqref{eq:rolling_stones} ist stückweise linear, sodass wir seine Abs Normal Form \eqref{eq:absNormalForm} angeben können mit
\[
c = \begin{pmatrix}
     -1\\
     1
    \end{pmatrix}
\quad
 Z = \begin{pmatrix}
      1 & 0 \\
      1 & 0
     \end{pmatrix}\quad
J = \begin{pmatrix}
      0&1\\
      -1 & 0
     \end{pmatrix}\quad
 Y = \begin{pmatrix}
      0 & 0\\
      -0.5  & 0.5
     \end{pmatrix}
\]
der Rest wird passend zur Dimension zu $0$ gesetzt. Für das Rolling Stones Beispiel lässt sich eine analytische Lösung zur ODE \eqref{eq:rolling_stones} für den Anfangswert $x_0=(1,1)$ angeben. Sie ist $2\pi+4$ periodisch, hat die Form
\begin{equation}
 x(t) = \begin{cases}
         1+\sin(t) 	& 0\leq t \leq \pi	\\
         1-(t-\pi) 	& \pi \leq t < \pi+2	\\ 
         -1 - \sin(2-t) 	& \pi+2\leq t<2\pi+2	\\
         t-3-2\pi	& 2\pi+2 \leq t<2\pi +4
        \end{cases}
\label{eq:analyticSolRolling}
\end{equation}
und wird in Figur \ref{fig:rollingStonesSolution} angegeben. Dabei sind die grau eingezeichneten Bereiche linear.

\subsection{Lösen der ODE}
% Die Konvergenz der verallgemeinerten impliziten Mittelpunktsregel 
\begin{figure}
\centering
\input{img/convergence_rolling_plot.tikz}
\caption{Konvergenz Rolling Stones im Intervall $[0,50]$ mit $x_0=(1,1)$}
\label{fig:rollingStonesConvergence}
\end{figure}


\begin{figure}[H]
\footnotesize 
\centering
% \quad
\begin{minipage}[b]{0.45\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_energy_error.tikz}
\caption{Rolling Stones Energieverlust im Intervall $[0,2\pi+4]$ mit $x_0=(1,1)$}
\label{fig:rollingStonesEnergyError}
\end{minipage}
\quad
\begin{minipage}[b]{0.45\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/convergence_rolling_romberg_plot.tikz}
\caption{Konvergenz der Romberg Extrapolation im Intervall $[0,50]$ mit $x_0=(1,1)$}
\label{fig:rollingStonesConvergenceRomberg}
\end{minipage}

\end{figure}


Die in Theorem \ref{thm:convergenceGenMidpoint} vorhergesagte Konvergenzordnung von $\mathcal O(h^2)$ erfüllt die verallgemeinerte implizite Mittelpunktsregel wie in Figur \ref{fig:rollingStonesConvergence} ersichtlich.
Dass die gewöhnliche implizite Mittelpunktsregel eine ebenso hohe Konvergenzrate besitzt ist darin begründet, dass sie im gegebenen Intervall nur $4$ mal pro $2\pi+4$ Periode Unglattheiten überquert, sonst jedoch normal konvergiert. Aufgrund dieser Unglattheiten entsteht für die implizite Mittelpunktsregel (IMP) kein glatter Konvergenzgraph, sondern sie springt. Demgegenüber konvergiert die verallgemeinerter implizite Mittelpunktsregel (GIMP) sehr stabil, da sie die Knicke der Funktion mit in ihre Berechnung einbezieht.
Insbesondere wird der Fehler der GIMP mit zunehmender Zeit (siehe Fig. \ref{fig:rollingStonesEOT}(a)) nur in den nicht affinen Abschnitten größer während in den affinen Abschnitten exakt gelöst wird, der lokale Fehler der IMP erhöht sich jedoch ständig, bedingt durch die Kinks und der zu großen Schrittweite. Der allgemeine Fehler ist dadurch für die IMP höher über die Zeit gesehen als der Fehler der GIMP (\ref{fig:rollingStonesEOT}(b)).

Da sich der rollende Stein reibungslos in unserem System bewegt, muss die analytische Lösung die komplette Energie erhalten, d.h. die potentielle Energie addiert mit der kinetischen $V(x) + \frac{1}{2}\dot x^2$ muss konstant sein. Das Bild \ref{fig:rollingStonesEnergyError} wurde mittels der summierten Variation der Energie 
\begin{equation}
 h \left[\sum_{l=0}^{T/h} \left( V(x_1^l) + \frac{1}{2} (x_2^l)^2 -\frac{1}{2}\right)^2\right]^{\sfrac{1}{2}}
 \label{eq:energyVariation}
\end{equation}
berechnet. Es ist deutlich zu erkennen, dass die Energie mit der GIMP deutlich besser erhalten bleibt als mit den klassischen Methoden, selbst für große Schrittweiten.
Dadurch führt die in Figur \ref{fig:rollingStonesConvergenceRomberg} durchgeführte Romberg Extrapolation zu sehr guten Ergebnissen. Da der Konvergenzgraph der GIMP stabil ist, lässt sich eine gute Extrapolation mit Konvergenzen der Ordnung 3 bis 4 erreichen. Demgegebenüber besitzt die Extrapolation der IMP keine höhere Konvergenzordnung als die der IMP selbst.
\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_error_over_time.tikz}
\caption*{(a) Am Zeitpunkt $t$}\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_error_over_time_all.tikz}
\caption*{(b) Summiert}
\end{minipage}
\caption{Rolling Stones Fehler über Zeit, \\$h=0.1,x_0=(1,1)$}
\label{fig:rollingStonesEOT}

\end{figure}

\subsection{Gradient des Kostenfunktionals}
Ähnliche Ergebnisse wie beim Lösen der ODE erwarten wir für die Integration des inhomogenen adjungierten Tangent Linear Models \eqref{eq:inhAdjEquation}. Falls nicht anders beschrieben werden im Folgenden immer die exakte Lösung des Rolling Stones Beispiel \eqref{eq:analyticSolRolling} als Obervierungsparameter $x_{\text{Obs}}$ mit der selben Diskretisierung wie die des ODE Lösers verwendet.

Beim Plot des Gradienten nach Anfangswerten wie in Figur \ref{fig:rollingGrad} ist zu erkennen, dass der Gradient berechnet mit der GIMP ebenfalls stabiler wirkt (Fig. \ref{fig:rollingGrad} links) als der Gradient berechnet mit der normalen Impliziten Mittelpunktsregel (Fig. \ref{fig:rollingGrad} rechts). Desweiteren existieren im Letzteren vereinzelt hohe Sprünge, welche durch nicht einbezogene Kinks erklärt werden können. 
\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_jac_da_gradient1.tikz}
% \caption{Rolling Stones Fehler über Zeit}
% \label{fig:rollingGrad1Gimp}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_jac_da_gradient1_imp.tikz}
% \caption{Rolling Stones Fehler über alle Komponenten}
% \label{fig:rollingGrad1Imp}
\end{minipage}
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_jac_da_gradient2.tikz}
% \caption{Ableitung des Kostenfunktional in erster Komponente von Rolling Stones }
% \label{fig:rollingGrad2Gimp}
\caption*{(a) GIMP}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_jac_da_gradient2_imp.tikz}
% \caption{Ableitung des Kostenfunktional in erster Komponente von Rolling Stones}
% \label{fig:rollingGrad2Imp}
\caption*{(b) IMP}
\end{minipage}
\caption{Ableitung des Kostenfunktionals beim Rolling Stones Beispiel, $I = [0,30],h=0.5$}
\label{fig:rollingGrad}
\end{figure}


Die Konvergenzplots \ref{fig:rollingStonesAdjoint} ergeben ähnliche Robustheitsresultate wie bei der Vorwärtsintegration. Figur \ref{fig:rollingStonesAdjoint}(a) stellt dabei die Konvergenz mit einer Funktion als Observierungsparameter dar. Das bedeutet, dass in jeder Verfeinerung ebenfalls die Observierungsparameter verfeinert wurden also $t_{\text{state}} = t_{\text{Obs}}$. Diskrete Observierungsparameter wurden bei Figur \ref{fig:rollingStonesAdjoint}(b) benutzt, sie blieben also bei jeder Verfeinerung des Gitters konstant. Die aus der GIMP berechneten Werte wurden mit einem Projektionsoperator vom Zustandsraum $X_{\text{State}}$ in den Raum der Observierungen $X_{\text{Obs}}$ projeziert, danach deren Differenz gebildet und wieder zurück nach $X_{\text{State}}$ projeziert.
% Die lineare Konvergenz der beiden Plots liefert 
\begin{figure}[H]
\centering
\input{img/rolling_adjoint_eq.tikz}
\caption{Rolling Stones Adjungierte Gleichung im Intervall $[0,40]$}
\label{fig:rolling_adjoint_eq}
\end{figure}
In Abbildung \ref{fig:rolling_adjoint_eq} zeigt die zu rückwärts zu integrierende rechte Seite. Dabei sind für $\dot \bar{x_2}$ deutlich Sprünge zu erkennen. Dadurch fällt die in Kapitel \ref{sec:adjointInclusion} vorhergesagte lineare Konvergenz aller Methoden auf.
% Selbst für einfachere Beispiele ohne Kinks ist die Konvergenz nicht schneller als linear.
% TODO: HERAUSFINDEN WARUM DAS SO IST


\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_convergence_adjoint_smooth.tikz}
\caption*{(a) Glatte Observierung}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_convergence_adjoint_discrete.tikz}
\caption*{(b) Diskrete Observierung}
\label{fig:rollingStonesAdjointDiscrete}
\end{minipage}
\caption{Rolling Stones Konvergenz $\nabla J(x_0)$\\ $I=[0,40],x_0=(0.5,0.5)$}
\label{fig:rollingStonesAdjoint}
\end{figure}

\subsection{Optimierung}
Die zu minimierende Funktion, das Kostenfunktional $J$, ist in Figur \ref{fig:rolling_costfunctional} gegeben. $J$ ist hierbei offensichtlich nicht konvex; es entsteht um den Punkt $(0,0)$ eine Art Spirale. Sein Minimum befindet sich am Punkt $(1,1)$, trivialerweise an der Stelle der Observierungsparameter. 
% Pfad der Iterationen
\begin{figure}[H]
\centering
\input{img/rolling_costfunctional.tikz}
\caption{Rolling Stones Kostenfunktional im Intervall $[0,30],h=0.5$}
\label{fig:rolling_costfunctional}
\end{figure}

Die Optimierung in diesem Beispiel wurden mit dem BFGS - Verfahren (siehe Algorithmus \ref{alg:bfgs}) durchgeführt. Dabei war in den Versuchen zu erkennen, dass die verallgemeinerte Methode ein besseres Konvergenzverhalten durch den glatteren Gradienten besitzt (Figur \ref{fig:rollingStonesOpt2}).

Für bestimmte Anfangswerte divergiert die klassische Methode wie in Figur \ref{fig:rollingStonesOpt1} wohingegen die Optimierung über GIMP konvergiert. 

Zusammenfassend ergibt sich für das Rolling Stones Beispiel ein insgesamt besseres Verhalten der verallgemeinerten Methoden im Gegensatz zu der klassischen impliziten Mittelpunktsregel.
\begin{figure}[H]
\footnotesize
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_opt2_cost.tikz}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_opt2_convergence.tikz}
\end{minipage}
\caption{Rolling Stones Data Assimilation Optimierung mit $x_0=(0,-1.45)$ auf dem Intervall $I = [0,20], h=0.2$}
\label{fig:rollingStonesOpt2}
\end{figure}

\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_opt1_cost.tikz}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/rolling_opt1_convergence.tikz}
\end{minipage}
\caption{Rolling Stones Data Assimilation Optimierung mit $x_0=(1.5,1.5)$ auf dem Intervall $I = [0,20], h=0.2$}
\label{fig:rollingStonesOpt1}
\end{figure}


\section{LC-Diode}
Das nächste Beispiel hat mehr Nähe zu praktischen Bezügen. In \cite{boeck2014experiments} eingeführt, betrachten wir einen LC-Schaltkreis in welchem wir den Widerstand durch eine Diode ausgewechselt haben, welches Unglätten in unsere Systemgleichungen bringen. Im Bild \ref{fig:lcDiode} ist eine schematische Darstellung des Problems dargestellt. 
\begin{figure}[H]
\centering
\input{img/lc_circuit.tikz}
\caption{LC Schaltkreis Diagramm}
\label{fig:lcDiode}
\end{figure}
Das beschreibende System von ODEs hat die Form
\[
 \begin{pmatrix}
  \dot x_1\\
  \dot x_2\\
  \dot x_3\\
 \end{pmatrix}
 = 
 \begin{pmatrix}
  1\\
  x_3\\
  -\left(x_2-CV(x_1) + g(Cx_3)\right)\frac{1}{LC}
 \end{pmatrix}
\]
wobei $x_1$ die Zeit, $x_2$ die Ladung des Kondensators $C$ und $x_3$ die elektrische Stromstärke des Gesamtsystems bezeichnet. Die Spannungsquelle wird mittels $V(t)=\sin(\omega t)$ simuliert und $g(z)$ modelliert mittels der stückweisen linearen Funktion 
\begin{equation}
 g(z) = \frac{z+|z|}{2\alpha} + \frac{z-|z|}{2\beta}  = \begin{cases}
                                                         \frac{z}{\alpha} & z\geq 0\\
                                                         \frac{z}{\beta}  & z<0
                                                        \end{cases}
\label{eq:lcOde}                                                       
\end{equation}
die Diode. Als Konstanten werden ähnliche Werte wie in echten Schaltkreisen verwendet
\[
 L= 10^{-6},~ C=10^{-13},~ \omega = 3\cdot 10^{9},~\alpha = 2,~\beta = 10^{-5}
\]
Als Anfangswert liegt kein Strom an, d.h. $x_1(0)  = x_2(0) = x_3(0) = 0$.

Im Gegensatz zu dem Rolling Stones Beispiel lässt sich dieses Problem nicht in einer Abs-Normal Form darstellen lässt, da es kein stückweise lineares Problem ist. Die in Theorem \ref{thm:quadrApproxPL} besagte Konvergenz zweiter Ordnung ist nun hier zu beachten, da sich das Modell nicht exakt Linearisieren lässt. Als weiteres numerisches Problem stellen sich die kleinen Konstanten $L,C$ und $\beta$ heraus, ebenso wie die kleine Schrittweite in einem Intervall von $[0, 10^{-8}]$. In der Lösung erhalten wir für die Ladung des Kondensators Größenordnungen von $10^{-13}$, welche an der Maschinengenauigkeit von $\approx 1.1\cdot 10^{-16}$ (double precision) grenzen.

\subsection{Lösen der ODE}
\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_solution.tikz}
\caption*{(a) Ladung des Kondensators}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_solution2.tikz}
\caption*{(b) Stromstärke}
\end{minipage}
\caption{LC-Diode Lösung der ODE}
\label{fig:lc_solution}
\end{figure}
Die Lösung der ODEs \eqref{eq:lcOde} wird in Bild \ref{fig:lc_solution} gezeigt. Zu sehen ist, dass der Kondensator sich initial in einem Zyklus auflädt und sich über die Zeit nach und nach entlädt und schließlich in einem periodischen Verhalten mündet. Die Trajektorie der Stromstärke ändert ihr Verhalten mit ihrem Vorzeichen. Desweiteren war es für die Implementierung notwendig, Abbruchtoleranzen von $10^{-14}$ zu wählen. 

\begin{figure}
\centering
\input{img/lc_convergence.tikz}
\caption{Konvergenz LC Diode im Intervall $[0,1.5\cdot 10^{-8}]$ mit $x_0=(0,0,0)$}
\label{fig:lcConvergence}
\end{figure}
Im Konvergenzgraph \ref{fig:lcConvergence} ist ein ähnliches Verhalten wie beim Rolling Stones Beispiel zu erkennen. Nachdem sich der Konvergenzgraph bis ca. $5\cdot 10^2$ Schritten ähnlich wie die implizite Mittelpunktsregel verhält, konvergiert er stabil mit Ordnung 2, während die Implizite Mittelpunktsregel bedingt durch die wenigen Kinks ebenfalls springt. Die explizite Mittelpunktsregel konvergiert unterhalb von $10^3$ Schritten nicht. 
Herauszustellen ist also, dass unsere Methoden selbst für nicht stückweise affine Beispiele stabile Konvergenzresultate erzielen.

Die Romberg Extrapolation gestaltet sich in diesem Beispiel schwierig. Zwar ist in Bild \ref{fig:lcRomberg} ein besseres Verhalten der Extrapolation für die GIMP zu erkennen, diese stößt jedoch an die Grenzen sobald sie sich der Maschinengenauigkeit nähert.  Die Konvergenzgeschwindigkeit betrug in diesem Beispiel nur $2.24$. Insbesondere im Abschnitt zwischen $2\cdot 10^{3}$ und $10^4$ ist jedoch eine deutlich exaktere Extrapolation der GIMP im Gegensatz zur Extrapolation der IMP zu beobachten. In diesem Intervall beträgt die Konvergenzgeschwindigkeit schon $3.89$. 
\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_romberg.tikz}
\caption*{(a) Gesamt}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_romberg2.tikz}
\caption*{(b) Ausschnitt}
\end{minipage}
\caption{Romberg Extrapolation}
\label{fig:lcRomberg}
\end{figure}



Beim Fehler über die Zeit ist genau zu erkennen, dass der Fehler sich an den Kinks erhöht. Die GIMP hat gegenüber den anderen Methoden den kleinsten Fehler. 
\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_error_over_time.tikz}
\caption*{(a) Am Zeitpunkt $t$}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_error_over_time_all.tikz}
\caption*{(b) Summiert}
\end{minipage}
\caption{LC Fehler über Zeit mit $N =1024 ,x_0 = [0,0,0]$}
\end{figure}
\subsection{Gradient und Optimierung}
In diesem Abschnitt werden wir die Lösung im Intervall $[0,10^{-8}]$ mit den Anfangswerten $x_0=(0,0,0)$ als Observierungsparameter benutzen. Da wir keine analytische Lösung berechnen können, wird eine Lösung mit einer erheblich exakteren Schrittweite ($4N$) als Vergleichswert verwendet.

Wie in Bild \ref{fig:lcAdjointConvergence} zu erkennen, konvergiert der Gradient bei der LC-Diode mit einer deutlich höheren Ordnung als im Rolling Stones Beispiel. Mit glatten Observierungsparametern ist der mit der GIMP berechnete Gradient von Anfang an zwei Größenordnungen genauer (Bild \ref{fig:lcAdjointConvergence}(b)), als der mit der IMP berechnete Gradient. 
\begin{figure}[H]
\footnotesize 
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_convergence_adjoint_discrete.tikz}
\caption*{(a) Diskrete Observierung}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_convergence_adjoint_smooth.tikz}
\caption*{(b) Glatte Observierung}
\end{minipage}
\caption{LC Diode Konvergenz $\nabla J(x_0)$, \\$I=[0,10^{-8}],x_0=(0,0,10^{-13})$}
\label{fig:lcAdjointConvergence}
\end{figure}

Die Optimierung über das Kostenfunktional, Bild \ref{fig:lcOpt}, verläuft mit der GIMP ebenfalls stabiler als mit der IMP. Man sieht, dass nach zirka 5 Schritten die Konvergenz zunimmt bis sie schließlich nach 19 Schritten konvergiert \ref{fig:lcOpt}(a). Die Berechnung mit der IMP konvergiert nicht so gleichmäßig wie die GIMP. Jedoch ist das Konvergenzverhalten abhängig vom  initiale gewählten Anfangswert. In Bild \ref{fig:lcOpt}(b) konvergieren die IMP und die GIMP fast gleichschnell.

\begin{figure}[H]
\footnotesize
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_opt1_convergence.tikz}
\caption*{(a) $x_0=(0,10^{-10},3\cdot 10^{-8})$}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/lc_opt2_convergence.tikz}
\caption*{(b) $x_0=(0,10^{-11},5\cdot 10^{-7})$}
\end{minipage}
\caption{LC Diode Data Assimilation Optimierung auf dem Intervall $I = [0,10^{-8}], N=512$}
\label{fig:lcOpt}
\end{figure}


\section{Shallow Water Equation}
% \subsection{Problemstellung}
Bisher wurde die stückweise Linearisierung nur mit gewöhnlichen Differentialgleichungen durchgeführt; Ziel dieses Beispieles ist es, diese Methoden auf eine partielle Differentialgleichungen anzuwenden und an ihr Data Assimilation zu betreiben.

Ein oft genutztes Beispiel, in dem Datenassimilierungsmethoden Anwendung finden ist die sogenannte Shallow Water Equation (vgl. \cite{zou,navon}). 
Diese partiellen Differentialgleichungen gehören zur Klasse der Saint-Venan Systemen und beschreiben eine Strömung einer Flüssigkeit in einem Gebiet unter Berücksichtigung diverser Nebenbedingungen wie Gravitationswellen, Bodenbegebenheit und Randgebiete. Eingeführt wurden sie bereits vor über 140 Jahren von Saint-Venant in \cite{saint1871theorie} und werden zur Simulation von Strömungen in Kanälen und Küstengebieten verwendet.

Im einfachsten eindimensionalen Fall ergibt sich das Saint-Venan System zu 
\begin{equation}
\begin{cases}
 h_t + (hu)_x = 0,\\
 (hu)_t + \left[\frac{(hu)^2}{h} + \frac{g}{2}h^2\right]_x = 0
\end{cases} 
\label{eq:swe}
\end{equation}
wobei $u$ die Geschwindigkeit, $hu$ den Wasserabfluss, $h$ die Höhe und $g=9.81 \sfrac{m}{s^2}$ die Gravitationskonstante bezeichnet.
Die Gleichung \eqref{eq:swe} wird 1-D Shallow Water Equation (SWE) genannt.
Diese kann durch Bodengegebenheiten, Reibung oder konvektiver Beschleunigung erweitert werden.
Eine interessante Eigenschaft dieser Gleichung ist, dass sie glatte stationäre Lösung zulässt, die die Bedingungen
\[
 hu = \text{const}, \quad \frac{u^2}{2}+gh = \text{const}
\]
erfüllen und unglatte stationäre Lösungen. Eine stationäre Lösung ist beispielsweise der "ruhende See"
\[
 u=0, \quad h=\text{const}
\]
Methoden, die diese Lösungen erhalten werden als ausgeglichen (well balanced) bezeichnet. Falls die Höhe des Systems $h \approx 0 $, sich also einem \textit{dry state} nähert, kann sie durch numerische Ungenauigkeiten kleiner Null werden, sodass die Berechnung abbricht, da die Eigenwerte der Jacobimatrix von \eqref{eq:swe} die Form $u\pm \sqrt{gh}$ haben.

Der Grundgedanke zur numerischen Lösung des Problemes besteht darin, die hyperbolische partielle Differentialgleichung \eqref{eq:swe} in ein System von ODEs umzuformen, sodass unsere Methoden darauf angewendet werden können.
Dazu nutzen wir die Finite Volumen Methode, welche unter Anderem von LeVeque in \cite[Abschnitt 4.]{leveque2002finite} ausführlich behandelt wird. Ähnlich wie Finite Differenzen werden Werte an diskreten Punkten in einem Gitter berechnet. Jedoch wird nicht der Punkt allein betrachtet, sondern die gesamten Gitterzellen, die sogenannten Volumen. In jedem dieser Volumen gilt der Erhaltungssatz für eine Größe; in unserem Beispiel die Höhe und der Wasserabfluss. Eine Veränderung der erhaltenden Größe kann nur durch Ab- oder Hinzufließen über den Rand der Gitterzelle geschehen; dieser wird auch als Flux bezeichnet. Dazu wird der Mittelwert der Erhaltungsgrößen in jeder Zelle berechnet. Damit entsteht eine Gleichung über die Veränderung der Größen in der Zeit, welche durch Löser für gewöhnliche Differentialgleichungen berechnet werden können.
% TODO: im LeVeque nachschauen und evtl. mit Theoremen ergänzen!

Kurganov bietet in \cite{kurganov2007second} ein robustes second order central upwind Finite Volumen Schema für die SWE an, die sowohl \textit{well balanced} ist als auch den \textit{dry state} behandelt. Es wurde genutzt, um die gewünschte Ortsdiskretisierung durchzuführen. Im Bild \ref{fig:sweSolution} ist die Lösung von \eqref{eq:swe} über die Zeit dargestellt.

\begin{figure}
 \centering
 \input{img/swe_plot.tikz}
 \caption{Lösung der 1-D SWE auf $I=[0,300]$, Länge $L=500,~\Delta x=10,~\Delta t=1$, periodischen Randbedingungen und normalverteiltem Anfangswert}
 \label{fig:sweSolution}
\end{figure}

% \cite{evans1998partial}
\subsection{FVM Schema und Unglattheiten}
\label{sec:fvmFluxEigen}
Das Schema von Kurganov aus \cite{kurganov2007second} benutzt ein uniformes Gitter mit $x_\alpha=\alpha\cdot \Delta x$ wobei $\Delta x$ den Abstand zum nächsten Gitterpunkt beschreibt. Das Intervall $I_j = [x_{j-\sfrac{1}{2}},x_{j+\sfrac{1}{2}}]$ sei die $j-$te finite Volumenzelle.
Eine zentrale Upwinding Semidiskretisierung von \eqref{eq:swe} ist das folgende System von ODEs
\[
 \frac{d}{dt} \bar U_j = - \frac{H_{j+\sfrac{1}{2}}-H_{j-\sfrac{1}{2}}}{\Delta x}
\]
wobei 
\[
 \bar U_j \approx \frac{1}{\Delta x}\int_{I_j} U(x,t) dx, \quad U:=(h,hu)^\tr
\]
den Mittelwert der Erhaltungsgrößen der j-ten Zelle beschreibt. Der numerische Fluss $H_{j+\sfrac{1}{2}}$ ist definiert als
\[
\begin{aligned}
 H_{j+\sfrac{1}{2}}(t) =& \frac{a^+_{j+\sfrac{1}{2}}F(U^-_{j+\sfrac{1}{2}}) - a^-_{j+\sfrac{1}{2}}F(U^+_{j+\sfrac{1}{2}})}{a^+_{j+\sfrac{1}{2}} -a^-_{j+\sfrac{1}{2}}} \\
 & +\frac{a^+_{j+\sfrac{1}{2}} \cdot a^-_{j+\sfrac{1}{2}}}{a^+_{j+\sfrac{1}{2}} -a^-_{j+\sfrac{1}{2}}}\left[U^+_{j+\sfrac{1}{2}} - U^-_{j+\sfrac{1}{2}}\right]
\end{aligned}
 \]
$F$ beschreibt die eigentliche rechte Seite der PDE
\[
 F(U) := \left( hu, \frac{(hu)^2}{h}  +\frac{g}{2} h^2 \right)^\tr
\]
und $U^\pm_{j+\sfrac{1}{2}}$ sind die linken/rechten Seiten einer Stückweise linearen Rekonstruktion von $U$ zum Zeitpunkt $t$
\[
 U^\pm_{j+\sfrac{1}{2}} = U_{j+\sfrac{1}{2} \pm \sfrac{1}{2}} + \frac{\Delta x}{2} \left(U_x\right)_{j+\sfrac{1}{2}\pm\sfrac{1}{2}}
\]
Die numerischen Ableitungen $(U_x)_j$ sind die Komponentenweisen Approximationen von $U_x(x_j ,t)$, welche durch nichtlineare \textit{Flux Limiter} berechnet werden, 
da ansonsten oszillatorischen Verhalten in der Lösung auftreten kann aufgrund von unstetigem Verhalten der Lösung und deren Approximation (\cite[Abschnitt 6.6]{leveque2002finite}).
Der verallgemeinerte MinMod Flux Limiter (\cite[(4.9)]{kurganov2000new}) wurde in der Implementierung benutzt und hat die Form
\[
 \left(U_x\right) = \text{minmod}\left(\theta \frac{\bar U_j- \bar U_{j-1}}{\Delta x},\frac{\bar U_{j+1}- \bar U_{j-1}}{2\Delta x},\theta\frac{ \bar U_{j+1}- \bar U_{j}}{\Delta x}  \right), \quad \theta \in [1,2]
\]
wobei minmod definiert ist als
\[
 \text{minmod}(z_1,z_2,\ldots) = \begin{cases}
                                  \min_j z_j & \text{falls }z_j>0~\forall j\\
                                  \max_j z_j & \text{falls }z_j<0~\forall j\\
                                  0 & \text{sonst}
                                 \end{cases}
 \]
 
Diesen kann man in max - min Repräsentation überführen, sodass wir die Funktion
 \begin{equation}
\text{minmod}(z_1,z_2,z_3) = \min(\max(z_1,z_2,z_3),0) + \max(\min(z_1,z_2,z_3),0)    
 \end{equation}

erhalten. Die Konstante $\theta$ wird, falls nicht anders beschrieben, in unseren Beispielen auf $\theta=1$ gesetzt.

Die rechts/linksseitigen lokalen Geschwindigkeiten $a^\pm_{j+\sfrac{1}{2}}$ werden aus dem größten und kleinsten Eigenwert der Jacobimatrix $\frac{\partial F}{\partial U}$ von \eqref{eq:swe} berechnet, wobei sie sich ergeben zu
\[
 \begin{aligned}
 a^+_{j+\sfrac{1}{2}} &= \max \left\lbrace u^+_{j+\sfrac{1}{2}} + \sqrt{gh^+_{j+\sfrac{1}{2}}},u^-_{j+\sfrac{1}{2}} + \sqrt{gh^-_{j+\sfrac{1}{2}}},0\right\rbrace\\
 a^-_{j+\sfrac{1}{2}} &= \max \left\lbrace u^+_{j+\sfrac{1}{2}} - \sqrt{gh^+_{j+\sfrac{1}{2}}},u^-_{j+\sfrac{1}{2}} - \sqrt{gh^-_{j+\sfrac{1}{2}}},0\right\rbrace
 \end{aligned}
\]
mit \begin{equation}
u=\frac{h(hu)}{h^2 + \varepsilon}
\label{eq:sweHuToU}
    \end{equation}
In den numerischen Experimenten ist $\varepsilon = \sfrac{1}{\Delta x^4}$ gewählt.
\begin{figure}
\footnotesize
\centering
\begin{minipage}[b]{\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/swe_grad_3d.tikz}
\caption*{(a) Höhe RHS}
\end{minipage}
% \quad
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/swe_grad_rhs_1.tikz}
\caption*{(b) Querschnitt Höhe $h$}
\end{minipage}
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/swe_grad_rhs_2.tikz}
\caption*{(c) Querschnitt Abfluss $hu$}
\end{minipage}
\caption{Rechte Seite der SWE über $I=[0,300], L=300, \Delta x=10, \Delta t = 1$}
\label{fig:sweGrad}
\end{figure}


Die Berechnung der Eigenwerte und der Ableitungen über den MinMod Limiter bringen die Unglattheiten auf der rechten Seite von \eqref{eq:swe}.  Der verallgemeinerte MinMod hat die switching depth $\nu = 3$ und erzeugt dadurch Knicke im computational graph. Ebenso die Eigenwertberechnung, welche eine switching depth von $\nu = 2$ erzeugt. Diese hohe switching depth führt in den Ergebnissen, wie in Abschnitt \ref{sec:absNormalForm} beschrieben, zu einer hohen Komplexität der Abs-Normal Form und numerischen Instabilitäten. In Figur \ref{fig:sweGrad} ist die Rechte Seite in der Zeit abgebildet. Man kann erkennen, dass Knicke auf der rechten Seite vorhanden sind, die auftreten, falls eine flache in eine steile Steigung übergeht und vice versa. Ebenso ist das oszillatorische Verhalten an den Knicken zu erkennen, welche zu Fehlern in der Zeitintegration mit klassichen Methoden führen.

\subsection{CFL Bedingung}
Bei der Wahl der Methode zur Zeitintegration wird in der Praxis zwischen expliziten und impliziten Verfahren abgewägt. Explizite Verfahren kommen besonders dort zum Einsatz, wo man eine möglichst feinschrittige Lösung benötigt. Für diesen Fall werden sogenannte higher order strong stability preserving (SSP) Runge Kutta Methoden, beispielsweise in \cite{gottlieb2003strong} vorgeschlagen, wozu auch das explizite Euler Verfahren gehört. Dazu werden Anforderungen an die Koeffizienten des Runge Kutta Verfahrens gestellt, sodass sie sich prinzipiell aus einfachen Euler Schritten zusammensetzen und ihre Konvergenzeigenschaft auf die des Expliziten Euler Verfahrens zurückgeführt werden kann (\cite[Lemma 1.1]{gottlieb2003strong}). Explizite Verfahren besitzen den Vorteil, dass bei der Berechnung kein lineares Gleichungssystem gelöst werden muss, welches unter Umständen sehr groß sein kann, auf jeden Fall aber einen höheren Rechenaufwand bedeutet.

Falls die Zeitschritte der Integration zu groß werden und damit über eine Nachbar - Volumenzelle hinaus zur nächsten propagiert werden, der numerische Flux des expliziten Verfahrens jedoch nur seine direkten Nachbarn betrachtet, kann dies zu einem instabilen Verhalten führen. Dies ist die Konsequenz der \textit{Courant-Friedrich-Levy Bedingung (CFL)}, welche eine notwendige Bedingung zur Stabilität des Verfahrens gibt (\cite[S.69]{leveque2002finite}):
\begin{theorem*}[CFL Bedingung]
  Eine numerische Methode kann nur dann konvergent sein, wenn der numerische Definitionsbereich der Abhängigkeiten dem wahren Definitionsbereich der Abhängigkeiten der PDE enthält; zumindest für $\Delta t, \Delta x\to 0$.
\end{theorem*}

Falls nun aber größere Schrittweiten benutzt werden müssen oder nur an der Lösung eines entfernten Zeitschrittes interessiert ist, werden in der Praxis implizite Verfahren benutzt, welche keiner CFL Bedingung unterliegen. Dies ist darin begründet, dass bei der Lösung des linearen Gleichungssystems der numerische Definitionsbereich der Abhängigkeiten der gesamte Definitionsbereich ist, welcher durch das lineare Gleichungssystem aneinander gekoppelt wird.

\subsection{Anfangswerte}
\begin{figure}[H]
\footnotesize
\centering
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/swe_initial_values.tikz}
\caption*{(a) $h$}
\end{minipage}
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\input{img/swe_initial_values_hu.tikz}
\caption*{(b) $hu$}
\end{minipage}
\caption{SWE Anfangswerte}
\label{fig:sweInitialValues}
\end{figure}
Die benutzten Anfangswerte $U_0 = (h_0,(hu)_0)$ sind in Bild \ref{fig:sweInitialValues} ersichtlich. Genauer ergeben sie sich zu
\[
\begin{aligned}
 h_0^{(1)} \sim n\cdot\mathcal N(0.5\cdot L,L) + 1 &&(hu)_0^{(1)}\sim n\cdot\mathcal N(0.5\cdot L,L) && \text{(Normalverteilt)}\\
 h_0^{(2)}(x)= \sin\left( \frac{2\pi k\cdot x}{L}\right) + 1.5 &&(hu)_0^{(2)}(x) = \cos\left( \frac{2\pi k\cdot x}{L}\right) + 1  && \text{(Schwingung)}
\end{aligned}
\]
Wobei $\mathcal N(\mu, \sigma^2)$ die Normalverteilung mit Erwartungswert $\mu$, Varianz $\sigma^2$ und Multiplikator $n$ ist. Falls $U_0^{(2)}$ gewählt wurde ist mit $k\in \R$ die Periode definiert. 
\subsection{Lösen der ODE}
Auch bei der Shallow Water Equation erhalten wir eine Konvergenz von $\mathcal O(\hat h^2)$, mit $\hat h$ als die Schrittweite. Wie im Bild \ref{fig:sweConvergence} ersichtlich, ist der Konvergenzgraph sehr stabil gegenüber der implizitem Mittelpunktsregel, wobei letztere stark springt.

Im Falle einer Schrittweite von $1.6\cdot 10^2$ divergiert die implizite Mittelpunktsregel, wobei die GIMP konvergiert. 
%TODO: WARUM?

Die Romberg Extrapolation konvergiert wie in den vorherigen Beispielen mit einer Ordnung von etwas weniger als $\mathcal O(\hat h^3)$. 

\begin{figure}
\centering
\input{img/swe_convergence.tikz}
\caption{Konvergenz SWE im Intervall $I = [0,50], L=50, \Delta x=10$ mit $U_0^{(1)},\theta=1$}
\label{fig:sweConvergence}
\end{figure}

\begin{figure}
\centering
\input{img/swe_convergence_romberg.tikz}
\caption{Konvergenz SWE Romberg im Intervall $I = [0,50], L=100, \Delta x=10$ mit $U_0^{(1)},\theta=1$}
\label{fig:sweConvergenceRomberg}
\end{figure}
Dass die verallgemeinerte Mittelpunktsregel eine bessere Integration liefert ist auch im Bild \ref{fig:sweErrorOverTime} ersichtlich, in dem der Fehler über die Zeit geplottet wurde. Während die GIMP anfangs schlechter integriert, besitzt sie, über einen längeren Zeitraum betrachtet, einen deutlich geringeren Fehler im Gegensatz zu den anderen Methoden.

\begin{figure}
\footnotesize
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_error_over_time.tikz}
\caption*{(a) Am Zeitpunk $t$}
\end{minipage}
%  \quad 
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_error_over_time_all.tikz}
\caption*{(b) Summiert}
\end{minipage}
\caption{SWE Fehler über Zeit mit $L=200,\Delta x=10,h = 1$ und $U_0^{(2)},k=2,\theta=1$}
\label{fig:sweErrorOverTime}
\end{figure}
Da in unserem System keine Reibung auftritt und periodische Randbedingungen haben, müssen die Summe der potentiellen und kinetischen Energie konstant sein. Nun können wir den Energieverlust über die Zeit mittels der Formel \eqref{eq:energyVariation} berechnen. Dabei wird die Höhe $h$ als potentielle und $\frac{1}{2}u^2$ als kinetische Energie betrachtet, wobei $u$ sich aus Formel \eqref{eq:sweHuToU} ergibt. Es entsteht (für $\hat h$ als Schrittweite)
\[
 \hat h \left[\sum_{l=0}^{T/\hat h} \left( h_l + \frac{1}{2} u_l^2 - \left(h_0 + \frac{1}{2} u_0^2\right)\right)^2\right]^{\sfrac{1}{2}}
\]
und als Energiefehler Bild \ref{fig:sweEnergyVariation}.
\begin{figure}
\centering
\input{img/swe_energy_variation.tikz}
\caption{Energievariation der SWE mit $L=50,\Delta x = 10, I = [0,25],\theta=1,U_0^{(2)},k=0.5$}
\label{fig:sweEnergyVariation}
\end{figure}
Es ist zu erkennen, dass in diesem Beispiel keine Verbesserung bzgl. des Energievariationsfehlers vorhanden ist.
\subsection{Gradient und Optimierung}
Bei der Konvergenz des Gradienten in Bild \ref{fig:sweConvergenceAdjoint} ist für $\theta=1$ kein besseres Verhalten der GIMP gegenüber der IMP feststellbar. Falls $\theta=2$ gesetzt wird (Bild \ref{fig:sweConvergenceAdjoint1}), so ergibt sich zwar keine bessere Konvergenzordnung der GIMP, jedoch hat sie einen kleineren Fehler.

Mit $\theta\approx 2$ ist der Limiter am wenigsten dissipativ, d.h. es entstehen keine neuen lokalen Extrema durch die Diskretisierung, währenddessen für $\theta\approx 1$ eine nichtoszillatorische Natur der approximativen Lösung sichert (\cite[Abschnitt 6]{kurganov2000new}).

Die Vermutung liegt deshalb nahe, dass die GIMP besser oszillatorisches Verhalten der Lösung behandelt, als die IMP.


\begin{figure}
\footnotesize
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_convergence_adjoint_discrete.tikz}
\caption*{(a) Am Zeitpunk $t$}
\end{minipage}
%  \quad 
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_convergence_adjoint_smooth.tikz}
\caption*{(b) Summiert}
\end{minipage}
\caption{SWE Konvergenz von $\nabla J$ mit $L=50,\Delta x=10$, $x_0=U_0^{(2)},k=0.5$, $x_{\text{Obs}} = U_0^{(2)}, k=2$ und $\theta=1$}
\label{fig:sweConvergenceAdjoint}
% \end{figure}
% 
% \begin{figure}
% \footnotesize
\quad\\[0.3cm]
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_convergence_adjoint_discrete1.tikz}
\caption*{(a) Diskrete Observierung}
\end{minipage}
%  \quad 
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_convergence_adjoint_smooth1.tikz}
\caption*{(b) Glatte Observierung}
\end{minipage}
\caption{SWE Konvergenz von $\nabla J$ mit $L=50,\Delta x=10$, $x_0=U_0^{(2)},k=0.5$, $x_{\text{Obs}} = U_0^{(2)}, k=2$ und $\theta=2$}
\label{fig:sweConvergenceAdjoint1}
\end{figure}


In Abbildung \ref{fig:sweConvergenceOpt} ist das Konvergenzresultat für eine Optimierung erkennbar. Beide Methoden konvergieren in etwa gleich schnell, wobei die Optimierung mit der GIMP etwas schneller konvergiert. Da beide Gradienten des Kostenfunktionals für die gegebenen Eingansparameter ein ähnlich schlechtes Konvergenzverhalten besitzen, war kein besseres Verhalten der GIMP zu erwarten. Auch in der Rekonstruktionen der Anfangswerte in Abbildung \ref{fig:sweOptInitValues} ist ersichtlich, dass beide Methoden ähnlich schlecht den ursprünglichen Anfangswert berechnen konnten.
% Da jedoch beide Methoden bei einem relativ großen Fehler abbrechen ist alles kacke
\begin{figure}[H]
\centering
\input{img/swe_convergence_opt.tikz}
\caption{SWE Konvergenz Optimierung mit Parametern $L=100,\Delta x=10$, $x_0=U_0^{(2)},k=0.5$, $x_{\text{Obs}} = U_0^{(2)}, k=1$ und $\theta=2$}
\label{fig:sweConvergenceOpt}
\end{figure}

\begin{figure}[H]
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_opt_initial_values.tikz}
\caption*{(a) $h$}
\end{minipage}
\begin{minipage}[b]{0.49\linewidth}
% \begin{minipage}[t][3cm][t]{5cm}
\centering
\input{img/swe_opt_initial_values_hu.tikz}
\caption*{(b) $hu$}
\end{minipage}
\caption{SWE Rekonstruktion der Anfangswerte mit $L=100,\Delta x=10$, $x_0=U_0^{(2)},k=0.5$, $x_{\text{Obs}} = U_0^{(2)}, k=1$ und $\theta=2$}
\label{fig:sweOptInitValues}
\end{figure}

