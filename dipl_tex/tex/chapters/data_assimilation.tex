\chapter{Datenassimilation}
\section{Problemstellung}
Die 4D variationelle Datenassimilation ist eine Methode, die im Zusammenhang mit hyperbolischen partiellen Differentialgleichungen steht, wie z.B. der Wellengleichung. Ziel ist es durch Steuerungsparameter, wie etwa Anfangswerte zum Zeitpunkt $t_0$, den Abstand eines Modells gegenüber der in der Zeit gemessenen Observierungsparameter minimal werden zu lassen. 

Um diese Methode herzuleiten folgen wir der Herangehensweise von Talagrand in \cite{talagrand1987variational} und nehmen zuerst ein Modell der Form
\begin{equation}
\label{eq:odemodel}
 \frac{dx}{dt} = F(x)\quad \text{mit} \quad x(0) = x_0 \quad \text{und}\quad F:\R^n\to \R^m
\end{equation}
  
welche eine Gewöhnliche Differentialgleichung beschreibt (vorheriges Kapitel). $x$ ist dabei die zeitabhängige Zustandsvariable aus dem Hilbertraum $\mathcal{E}$, welche die Entwicklung des Systems zu einem Zeitpunkt $t\in \R$ beschreibt. 
$x_0 \in \R^m$ sei der Anfangswert, über den wir steuern wollen.
Die Observierungsparameter $\xobs$ sind diskrete Werte, welche Orts - und Zeitabhängig und über ein Interval $[0,T]$ verteilt sind. Da $\xobs$ diskret ist wird eine Projektion $C:\Xstate\to \Xobs$ benötigt, welche $x$ aus dem Zustandsraum $\Xstate$ in den Observierungsraum $\Xobs$ abbildet. Üblicherweise ist diese Funktion eine Auswertungsfunktion von $x$ an den Stellen $\tobs$.

Als Kostenfunktional dient die $L^2$ - Norm der Differenz der Lösung $x$ von \eqref{eq:odemodel} zu $\xobs$ über ein gegebenes Zeitinterval $[0,T]$
\begin{equation}
\label{eq:costfunctional}
 J(x_0) = \frac{1}{2}\int_0^T H(x(t),t)dt= \frac{1}{2}\int_0^T \|C\cdot x(V)-\xobs\|^2dt
\end{equation}
Die Skalarwertige Funktion $J$ ist damit jene Funktion, die das Maß zwischen der Lösung $x$ und den Observierungsparametern $\xobs$ darstellt.
Man beachte, dass das Zielfunktional nur abhängig vom Anfangswert $x_0$ ist. Das Ziel besteht darin, $x_0 = x_0^*$ zu finden, sodass
\[
 J(x_0^*) = \inf_{x_0} J(x_0) 
\]
minimiert wird. Bei den meisten Optimierungsmethoden benötigt man daher den Gradienten der zu minimierenden Funktion $J$.

\section{Adjungierte Operatoren}
Für höhere Dimensionen ist die Bestimmung dieses Gradienten jedoch auf klassischem Weg numerisch sehr komplex. Dazu würde man jede Komponente unseres Systems \eqref{eq:odemodel} stören, das Modell integrieren und das Kostenfunktional für alle Komponenten ausrechnen. Aus der Störung würde sich der Gradient $\partial J/\partial x_0$ ergeben. Mit Hilfe des adjungierten Operators kann die numerische Komplexität der Berechnung des Gradienten auf ein weniges Vielfaches des integrierens von \eqref{eq:odemodel} vermindert werden.

Eine allgemeine Einführung zur Theorie von adjungierten Gleichungen wurde beispielsweise von Cacuci \cite{cacuci1981sensitivity} gegeben, während Talagrand in \cite{talagrand1987variational} adjungierte Operatoren auf Hilberträume einschränkt und mit Diesen die Datenassimilation einführt.
Für die folgenden Betrachtungen sind Hilberträume von besonderer Wichtigkeit. Sämtliche Lösungstheorie sowohl bei gewöhnlichen als auch bei partiellen Differentialgleichungen basieren auf den speziellen Eigenschaften dieser Räume.

Ein Hilbertraum ist ein reller oder komplexer Vektorraum mit einem Skalarprodukt, der bzgl. der vom Skalarprodukt induzierten Norm vollständig ist. Da $\R$ vollständig ist, ist jeder endlichdimensionale Vektorraum versehen mit einem Skalarprodukt vollständig. In der numerischen Praxis sind damit sämtliche folgenden Schritte valide. 
Ein Ableitungsbegriff auf Hilberträumen ist die sogenannte Gâteaux Ableitung.
\begin{definition}[Gâteaux Ableitung]
 Seien $X$ und $Y$ Banachräume, $U \subset X$ offen und $F:X\to Y$. Das Gâteaux Differential $\gatdiff F(u;\psi)$ von F an $u\in U$ in Richtung $\psi\in X$ ist definiert als
 \[
  \gatdiff F(u;\psi) = \lim_{\tau \to 0} \frac{F(x+\tau \psi) - F(x)}{\tau} = \frac{d}{d\tau} F(u+\tau\psi)\biggr\rvert_{\tau = 0}
 \]
 falls der Grenzwert existiert. Existiert der Grenzwert für alle $\psi\in X$, dann heißt $F$ Gâteaux-Differenzierbar in $u$.
 %TODO erwähnen dass das Ding im endlichdimensionalen zurückfällt auf bekannte Ableitung
\end{definition}
Die folgenden Eigenschaften von Hilberträumen sind besonders nützlich für nachfolgende Betrachtungen.
%TODO Beweisreferenz hinzufügen
\begin{proposition}[Adjungierte Operatoren]
\label{prop:adjoints}
\begin{enumerate}
 \item Sei $\Hil$ ein Hilbertraum mit Skalarprodukt $\langle \cdot,\cdot \rangle_\Hil$ und $v:\Hil \to \R,~ v\mapsto J(v)$ eine differenzierbare skalare Funktion definiert auf $\Hil$. Das Differential $\gatdiff J$ von $J$ kann an jedem Punkt von $\Hil$ ausgedrückt werden als
 \begin{equation}
 \label{eq:diffInnerProduct}
  \gatdiff J = \langle \nabla_v J, \gatdiff v\rangle_\Hil
 \end{equation}
 $\nabla_v J$ ist hierbei der eindeutige Gradient von $J$ in $v$. Wenn $\Hil$ endlichdimensional und durch orthonormale Koordinaten $v_i$ beschrieben ist, dann sind die Komponenten von $\nabla_v J$ gerade $\partial J/\partial v_i$.
\item Sei $\Gil$ ein anderer Hilbertraum mit Innenprodukt $\langle \cdot,\cdot \rangle_\Gil$ und $L$ ein stetiger linearer Operator von $\mathcal{G}\to \Hil$. Dann existiert ein eindeutiger stetiger linearer Operator $L^*$ von $\Hil\to\Gil$, sodass für alle $u\in \Gil$ und $v\in \Hil$ gilt
\begin{equation}
\label{eq:adjointInnerProduct}
\langle v,Lu\rangle_\Hil =  \langle L^*v,u \rangle_\Gil
\end{equation}
$L^*$ heißt der \textit{adjungierte Operator} von $L$. Im Falle das $\Hil$ und $\Gil$ endlichdimensional und durch orthonormale Koordinaten beschrieben sind, ist $L^*$ gerade die transponierte Matrix von $L$
\end{enumerate} 
\end{proposition}

Seien $\Hil,~\Gil$ und $J$ wie in Proposition \ref{prop:adjoints} eingeführt. Betrachtet man jetzt eine Funktion $u:\Gil \to \Hil, ~u\mapsto v = G(u)$ folgt, dass sich $J(v) = J(G(u))$ zu einer zusammengesetzte Funktion von $u$ ergibt. Nun ist 
\begin{equation}
\label{eq:diffCompoundFunction}
\gatdiff v = \gatdiff(G(u)) = G'\gatdiff u
\end{equation}
wobei $G'$ die Ableitung von $G$ bzgl. $u$ und ein linearer Operator von $\Gil\to\Hil$ ist. Wenn man nun \eqref{eq:diffCompoundFunction} in \eqref{eq:diffInnerProduct} einsetzt erhält man 
\begin{equation}
 \gatdiff J = \langle \nabla_v J,G'\gatdiff u\rangle_\Hil \overset{\eqref{eq:adjointInnerProduct}}{=}  \langle G'^* \nabla_v J,\gatdiff u\rangle_\Gil \overset{n.V.}{=} \langle \nabla_u J,\gatdiff u\rangle_\Gil 
\end{equation}
wobei $G'^*$ der adjungierte Operator von $G'$ beschreibt. Damit ist 
\begin{equation}
\label{eq:adjointEssential}
\nabla_u J = G'^*\nabla_v J
\end{equation} 
Die Gleichung \eqref{eq:adjointEssential} stellt die Basis für adjungierte Operatoren und insbesondere für die Datenassimilation dar. Sie bietet ein besonders effizienten Weg $\nabla_u J$ numerisch zu berechnen. Angewandt auf unser Problem wäre $u\mapsto v =G(u) $ eine Integration eines numerischen Modells. Aus einem Integrierer der $G'^*w$ für ein gegebens $w$ berechnen kann, kann nun aus $\nabla_v J$ mittels \eqref{eq:adjointEssential} einfach $\nabla_u J$ errechnet werden. 
% Wenn $J$ einfach gewählt wurde, lässt sich daraus $\nabla_v J$ ebenfalls einfach berechnen. 
Um $\nabla_u J$ zu berechnen, muss zuerst $v = G(u)$ berechnet werden, danach $\nabla_v J$ und schlussendlich wieder $\nabla_u J$ durch \eqref{eq:adjointEssential}.

\section{Anwendung auf die Datenassimilation}
Um vorherige Betrachtungen anwenden zu können wird die Gâteaux Ableitung von \eqref{eq:costfunctional} zu einem Anfangswert $v$ gebildet
\begin{equation}
\label{eq:gatcost}
\begin{aligned}
 \gatdiff J(v)  &= \lim_{\tau\to 0} \frac{1}{\tau}\int_0^T H(x+\tau\gatdiff x)-H(x) dt\\
	    &= \lim_{\tau\to 0} \frac{1}{\tau}\int_0^T \int_0^1 \frac{d}{ds}H(x+s\tau\gatdiff x) dt\\
	    &= \lim_{\tau\to 0} \frac{1}{\tau}\int_0^T \int_0^1 \nabla_x H(x+s\tau\gatdiff x)\cdot \gatdiff x dt\\
	    &= \int_0^T \nabla_x H(x)\cdot \gatdiff x dt\\
	    &= \int_0^T \langle \nabla_x H(x), \gatdiff x \rangle dt\\  
\end{aligned}
\end{equation}
wobei $\nabla_x H(t)$ der Gradient von $H(x,t)$ ausgewertet an der Stelle $(x(t),t)$ und $\gatdiff x$ die Gâteaux-Ableitung von $x$, also eine Störung. beschreibt. Diese hat die Form
\begin{equation}
\label{eq:tlm}
\begin{aligned}
  \frac{d \gatdiff x}{dt} &= \lim_{\tau\to 0}\frac{F(x+\tau \gatdiff x)-F(x)}{\tau}\\
			 &= \frac{d F(x+\tau\gatdiff x)}{d\tau} \Bigg\rvert_{\tau=0}\\
			 &= F'(t)\gatdiff x
\end{aligned} 
\end{equation}

und wird als \textit{Tangent Linear Model} bezeichnet. $F'(t)$ ist der Operator, der durch differenzieren von $F(x,t)$ nach $x$ ausgewertet an $x(t)$ entsteht. Da \eqref{eq:tlm} homogen und linear ist, hängt ihre Lösung linear vom Anfangswert $v$ ab und es gilt
\begin{equation}
\label{eq:resolvent}
 \gatdiff x(t) = R(t,0)v
\end{equation}
%TODO Referenz auf ODE theory einfügen
Wie allgemein bekannt ist, ist $R(t,0)$ ein wohldefinierter linearer Operator. Dieser wird als \textit{Resolvent} bezeichnet (siehe dazu REFERENZ NOCH EINF*GEN). Er besitzt die folgenden Eigenschaften
\begin{align}
  R(t,t) &= I\\
  \frac{\partial}{\partial t}R(t,t') &= F'(t)R(t,t') \text{ für alle } t,t>0
\end{align}
Nun kann Gleichung \eqref{eq:gatcost} mittels \eqref{eq:resolvent} und deren adjungiertem Operator $R^*(t,0)$ umgeschrieben werden zu
\begin{equation*}
\begin{aligned}
 \gatdiff J &= \int_0^T \langle \nabla_x H(t), R(t,0)v\rangle dt \\
	    &= \int_0^T \langle R^*(t,0) \nabla_x H(t), v\rangle dt \\
	    &= \left\langle \int_0^T R^*(t,0) \nabla_x H(t) dt, v\right\rangle \\
\end{aligned}
\end{equation*}
und es ergibt sich wie in den Betrachtungen zu Gleichung \eqref{eq:adjointEssential} %TODO die Anfangswertbedinungen einheitlich bennen v -> \gatdiff u oder so
\begin{equation}
\label{eq:gradCostFunctional}
 \nabla_v J = \int_0^T R^*(t,0) \nabla_x H(t) dt
\end{equation}
Der adjungierte Operator $R^*(t,0)$ des Resolventen $R(t,0)$ wird durch die Lösung des adjungierten Tangent Linear Models von \eqref{eq:tlm} 
\begin{equation}
\label{eq:adjointtlm}
 -\frac{d \gatdiff'x}{dt} = F'^*(t)\gatdiff'x
\end{equation}
beschrieben, wobei $\gatdiff'x$ Element von $\Hil$ und $F'^*(t)$ die Adjungierte von $F'(t)$ ist. 
Sei $S(t,t')$ der Resolvent von \eqref{eq:adjointtlm} zwischen den Zeitpunkten $t$ und $t'$. Für zwei Lösungen $\gatdiff x$ und $\gatdiff' x$ des Tangent Linear Models \eqref{eq:tlm} und deren Adjungierte \eqref{eq:adjointtlm} ist das innere Produkt $\langle \gatdiff x,\gatdiff' x\rangle$ konstant, da das innere Produkt der Ableitungen nach $t$ verschwindet
\begin{equation}
\begin{aligned}
 \frac{d}{dt}\langle \gatdiff x(t),\gatdiff' x(t)\rangle  &= \left\langle \frac{d \gatdiff x}{dt}(t) ,\gatdiff' x\right\rangle +\left\langle \gatdiff x,\frac{d\gatdiff ' x}{dt}(t)\right\rangle \\
 &= \langle F'(t)\gatdiff x(t),\gatdiff 'x(t)\rangle - \langle \gatdiff x(t),F'^*(t)\gatdiff' x(t)\rangle\\
 &= 0
\end{aligned}
\end{equation}
Daraus folgt unmittelbar, dass die Lösungen zweier Anfangswerte $y,y' \in \Hil$ die Gleichung
\begin{equation*}
\langle R(t',t)y,y'\rangle = \langle y,S(t,t')y'\rangle 
\end{equation*}
erfüllen, da die Lösung des Tangent Linear Models \eqref{eq:tlm} für die Anfangsbedingung $y$ am Zeitpunkt $t$ gerade den Wert $R(t',t)y$ zum Zeitpunkt $t'$ und die Lösung des adjungierten Tangent Linear Models \eqref{eq:adjointtlm} für die Anfangsbedingung $y'$ am Zeitpunkt $t'$ den Wert $S(t,t')y'$ zum Zeitpunkt $t$ annimmt. Da dies für alle $y,y'$ gilt, ist $S(t,t')$, der Resolvent der adjungierten Differentialgleichung \eqref{eq:adjointtlm}, der adjungierte Operator von $R(t',t)$, der Resolvent des Tangent Linear Models \eqref{eq:tlm}.
Für den Gradienten des Kostenfunktionals \eqref{gradCostFunctional} entsteht somit
\begin{equation}
 \nabla_v J = \int_0^T S^(0,t) \nabla_x H(t) dt
\end{equation}


% Zu einem gegebenen Anfangswert $v$ und der dazugehörigen Lösung $x(t)$ von \eqref{eq:odemodel} sieht die Variation erster Ordnung folgendermaßen aus
% \[
%  \delta J = \int_0^T \langle \nabla_x H(t), \delta x(t)\rangle dt
% \]






% Die variationelle Datenassimilation besteht grundsätzlich aus drei Schritten
% \begin{enumerate}
%  \item ODE über Interval $[0,T]$ lösen
%  \item Gradient des Kostenfunktionals errechnen über adjungiertes Modell
%  \item Minimierungsverfahren 
% \end{enumerate}